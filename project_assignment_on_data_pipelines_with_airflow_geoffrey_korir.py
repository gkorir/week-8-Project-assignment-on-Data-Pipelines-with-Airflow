# -*- coding: utf-8 -*-
"""Project assignment on Data Pipelines with Airflow Geoffrey Korir.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kXJ9Db1d0AzsyR2XCBJrOHa57GHk2y8F
"""

from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta
import pandas as pd
from sqlalchemy import create_engine
import logging

default_args = {
    'owner': 'MTN Rwanda',
    'depends_on_past': False,
    'start_date': datetime(2023, 4, 18),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG('data_pipeline', default_args=default_args, schedule_interval='@daily')

def extract_data():
    # extract data from CSV files
    customer_data = pd.read_csv('customer_data.csv')
    order_data = pd.read_csv('order_data.csv')
    payment_data = pd.read_csv('payment_data.csv')
    
    # load the CSV data into Pandas dataframes for later transformation
    return customer_data, order_data, payment_data

def transform_data(customer_data, order_data, payment_data):
    # customer_data, order_data, payment_data = extract_data()
    
    # convert date fields to the correct format using pd.to_datetime
    customer_data['date_of_birth'] = pd.to_datetime(customer_data['date_of_birth'])
    order_data['order_date'] = pd.to_datetime(order_data['order_date'])
    payment_data['payment_date'] = pd.to_datetime(payment_data['payment_date'])
    
    # merge customer and order dataframes on the customer_id column
    customer_order_data = pd.merge(customer_data, order_data, on='customer_id')
    
    # merge payment dataframe with the merged dataframe on the order_id and customer_id columns
    payment_customer_order_data = pd.merge(payment_data, customer_order_data, on=['order_id', 'customer_id'])
    
    # drop unnecessary columns like customer_id and order_id.axis = 1 means columns and not rows to be dropped
    payment_customer_order_data.drop(['customer_id', 'order_id'], axis=1, inplace=True)
    
    # group the data by customer and aggregate the amount paid using sum
    customer_lifetime_value = payment_customer_order_data.groupby(['email', 'country', 'gender', 'date_of_birth']).agg(total_amount_paid=('amount', 'sum'), number_of_orders=('product', 'count')).reset_index()
    
    # create a new column to calculate the total value of orders made by each customer
    customer_lifetime_value['total_order_value'] = customer_lifetime_value['total_amount_paid'] / customer_lifetime_value['number_of_orders']
    
    # calculate the customer lifetime value using the formula CLV = (average order value) x (number of orders made per year) x (average customer lifespan)
    customer_lifetime_value['lifespan'] = (pd.Timestamp.today() - customer_lifetime_value['date_of_birth']).dt.days / 365.25
    customer_lifetime_value['average_order_value'] = customer_lifetime_value['total_order_value']
    customer_lifetime_value['clv'] = customer_lifetime_value['average_order_value'] * customer_lifetime_value['number_of_orders'] * customer_lifetime_value['lifespan']
    
    return customer_lifetime_value
    
#we wrap the code that loads the data into a try/except block
def load_data(transformed_data):
    try:
        #transformed_data = transform_data()

        # load the transformed data into Postgres database
        engine = create_engine('postgresql://username:password@localhost:5432/dbname')
        connection = engine.connect()
        table_name = 'customer_data_clv'

        # create table if it doesn't exist
        if not engine.has_table(table_name):
            transformed_data.iloc[0:0].to_sql(table_name, engine, if_exists='replace', index=False)

        # insert data into table
        transformed_data.to_sql(table_name, connection, if_exists='append', index=False)

        connection.close()
        
        logging.info('Data loaded into Postgres database')
        
    except Exception as e:
        logging.error(f'Error loading data into Postgres database: {e}')
        raise


with dag:
    
    extract_data_task = PythonOperator(
        task_id='extract_data',
        python_callable=extract_data
    )

    transform_data_task = PythonOperator(
        task_id='transform_data',
        python_callable=transform_data
    )

    load_data_task = PythonOperator(
        task_id='load_data',
        python_callable=load_data
    )

    # define dependencies
    extract_data_task >> transform_data_task >> load_data_task